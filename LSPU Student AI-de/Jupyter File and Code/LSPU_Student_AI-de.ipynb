{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Step 0: Install Required Libraries**"
      ],
      "metadata": {
        "id": "fOMWnMEDWc7n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWt2z5sLIQmt"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets sentencepiece accelerate scikit-learn nltk torch tqdm numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Import Libraries and Download NLTK Data**"
      ],
      "metadata": {
        "id": "oKxZtUWDWiCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    T5Tokenizer,\n",
        "    T5ForConditionalGeneration,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "HehQ_6CwJ0L8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Define Data Loading and Preprocessing Functions**"
      ],
      "metadata": {
        "id": "38UxJNi2WoGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_data(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    augmented_data = []\n",
        "    for item in data:\n",
        "        augmented_data.append(item)\n",
        "\n",
        "        question = item['question']\n",
        "        answer = item['answer']\n",
        "\n",
        "        if question.lower().startswith('what is'):\n",
        "            new_question = 'Could you tell me' + question[7:]\n",
        "            augmented_data.append({'question': new_question, 'answer': answer})\n",
        "\n",
        "        if not question.lower().startswith('can you'):\n",
        "            new_question = f\"Can you explain {question.lower()}\"\n",
        "            augmented_data.append({'question': new_question, 'answer': answer})\n",
        "\n",
        "    return augmented_data\n",
        "\n",
        "\n",
        "def create_dataset(data):\n",
        "    questions = [item['question'] for item in data]\n",
        "    answers = [item['answer'] for item in data]\n",
        "\n",
        "    train_questions, test_questions, train_answers, test_answers = train_test_split(\n",
        "        questions, answers, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    test_questions, val_questions, test_answers, val_answers = train_test_split(\n",
        "        test_questions, test_answers, test_size=0.5, random_state=42\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'train': Dataset.from_dict({'question': train_questions, 'answer': train_answers}),\n",
        "        'test': Dataset.from_dict({'question': test_questions, 'answer': test_answers}),\n",
        "        'validation': Dataset.from_dict({'question': val_questions, 'answer': val_answers})\n",
        "    }"
      ],
      "metadata": {
        "id": "PAoTYexUJ0TA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Define Model Training Functions**"
      ],
      "metadata": {
        "id": "tSeC1bsrW1K7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples, tokenizer, max_length=384):\n",
        "    inputs = [f\"answer question: {q.strip()}\" for q in examples['question']]\n",
        "    targets = [a.strip() for a in examples['answer']]\n",
        "\n",
        "    model_inputs = tokenizer(\n",
        "        inputs,\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    labels = tokenizer(\n",
        "        targets,\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "DnqzsX1EJ0Zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Define Custom Trainer with Metrics**"
      ],
      "metadata": {
        "id": "HTe_tvG0xERc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTrainer(Trainer):\n",
        "    def compute_metrics(self, eval_pred):\n",
        "        predictions, labels = eval_pred\n",
        "\n",
        "        decoded_preds = self.tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "        decoded_labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "        binary_preds = [1 if p == l else 0 for p, l in zip(decoded_preds, decoded_labels)]\n",
        "        binary_labels = [1] * len(decoded_labels)\n",
        "\n",
        "        return {\n",
        "            'accuracy': accuracy_score(binary_labels, binary_preds),\n",
        "            'f1': f1_score(binary_labels, binary_preds, average='binary'),\n",
        "            'recall': recall_score(binary_labels, binary_preds, average='binary'),\n",
        "            'precision': precision_score(binary_labels, binary_preds, average='binary')\n",
        "        }"
      ],
      "metadata": {
        "id": "c-TFkZWVxEks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Define Model Training Function**"
      ],
      "metadata": {
        "id": "VStAbShnW03m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(datasets, model_name=\"t5-base\", output_dir=\"./results\"):\n",
        "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "    tokenized_datasets = {\n",
        "        split: datasets[split].map(\n",
        "            lambda x: preprocess_function(x, tokenizer),\n",
        "            batched=True,\n",
        "            remove_columns=datasets[split].column_names\n",
        "        )\n",
        "        for split in datasets.keys()\n",
        "    }\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=50,\n",
        "        learning_rate=5e-5,\n",
        "        per_device_train_batch_size=6,\n",
        "        per_device_eval_batch_size=6,\n",
        "        num_train_epochs=17,\n",
        "        weight_decay=0.01,\n",
        "        save_total_limit=2,\n",
        "        logging_steps=10,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        greater_is_better=False,\n",
        "        gradient_accumulation_steps=2,\n",
        "        warmup_steps=100,\n",
        "        fp16=True\n",
        "    )\n",
        "\n",
        "    trainer = CustomTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_datasets['train'],\n",
        "        eval_dataset=tokenized_datasets['validation'],\n",
        "        tokenizer=tokenizer,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    return model, tokenizer, trainer"
      ],
      "metadata": {
        "id": "ef18FpWGJ0hQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6: Define Enhanced Chatbot Class**"
      ],
      "metadata": {
        "id": "PFQn0l4_XGSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedChatbot:\n",
        "    def __init__(self, model_path):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(model_path).to(self.device)\n",
        "        self.model.eval()\n",
        "        self.fallback_response = \"I'm sorry, I don't have information on that. Can you please rephrase or ask a different question?\"\n",
        "        self.predefined_responses = {\n",
        "            \"hi\": \"Hello! How can I assist you today?\",\n",
        "            \"hello\": \"Hello! How can I assist you today?\",\n",
        "            \"hey\": \"Hello! How can I assist you today?\",\n",
        "            \"thanks\": \"You're welcome! Feel free to ask more questions.\",\n",
        "            \"thank you\": \"You're welcome! Feel free to ask more questions.\",\n",
        "            \"bye\": \"Goodbye! Have a great day!\",\n",
        "            \"goodbye\": \"Goodbye! Have a great day!\",\n",
        "            \"thankyou\": \"You're welcome! Feel free to ask more questions.\"\n",
        "        }\n",
        "\n",
        "    def preprocess_question(self, question):\n",
        "        question = question.lower().strip()\n",
        "        question = re.sub(r'[^a-z0-9\\s?.!,]', '', question)\n",
        "        if not question.endswith('?'):\n",
        "            question += '?'\n",
        "        return question\n",
        "\n",
        "    def generate_response(self, question, max_length=256, num_beams=5, confidence_threshold=0.5):\n",
        "        if question.lower() in self.predefined_responses:\n",
        "            return self.predefined_responses[question.lower()]\n",
        "\n",
        "        processed_question = self.preprocess_question(question)\n",
        "        input_text = f\"answer question: {processed_question}\"\n",
        "\n",
        "        inputs = self.tokenizer(\n",
        "            input_text,\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=max_length,\n",
        "            truncation=True,\n",
        "            padding=True\n",
        "        ).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                inputs[\"input_ids\"],\n",
        "                max_length=max_length,\n",
        "                num_beams=num_beams,\n",
        "                length_penalty=1.0,\n",
        "                early_stopping=True,\n",
        "                no_repeat_ngram_size=2,\n",
        "                temperature=0.7,\n",
        "                output_scores=True,\n",
        "                return_dict_in_generate=True\n",
        "            )\n",
        "\n",
        "        logits = outputs.scores[0]\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        confidence = probs.max().item()\n",
        "\n",
        "        response = self.tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
        "\n",
        "        if confidence < confidence_threshold:\n",
        "            return self.fallback_response\n",
        "\n",
        "        return response"
      ],
      "metadata": {
        "id": "MDMRb4wSJ0oH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 7: Evaluation Function**"
      ],
      "metadata": {
        "id": "Berso_gaXNp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(chatbot, test_dataset):\n",
        "    predictions = []\n",
        "    references = []\n",
        "\n",
        "    for item in tqdm(test_dataset):\n",
        "        question = item['question']\n",
        "        reference = item['answer']\n",
        "        prediction = chatbot.generate_response(question)\n",
        "\n",
        "        predictions.append(prediction)\n",
        "        references.append(reference)\n",
        "\n",
        "    correct_predictions = [1 if p == r else 0 for p, r in zip(predictions, references)]\n",
        "    binary_labels = [1] * len(references)\n",
        "\n",
        "    metrics = {\n",
        "        'accuracy': accuracy_score(binary_labels, correct_predictions),\n",
        "        'f1': f1_score(binary_labels, correct_predictions, average='binary'),\n",
        "        'recall': recall_score(binary_labels, correct_predictions, average='binary'),\n",
        "        'precision': precision_score(binary_labels, correct_predictions, average='binary')\n",
        "    }\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "WbEFh8zpJ8Fj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 8: Model Training and Evaluation**"
      ],
      "metadata": {
        "id": "jJPQzoXRXYoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate():\n",
        "    print(\"Loading and preprocessing data...\")\n",
        "    data = load_and_preprocess_data('LSPU-Student-Handbook-dataset.json')\n",
        "    datasets = create_dataset(data)\n",
        "\n",
        "    print(\"\\nTraining model...\")\n",
        "    model, tokenizer, trainer = train_model(datasets)\n",
        "\n",
        "    print(\"\\nSaving model...\")\n",
        "    output_dir = \"./enhanced-t5-chatbot\"\n",
        "    model.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    chatbot = EnhancedChatbot(output_dir)\n",
        "\n",
        "    print(\"\\nEvaluating model on test dataset...\")\n",
        "    metrics = evaluate_model(chatbot, datasets['test'])\n",
        "    print(\"\\nModel Metrics:\")\n",
        "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
        "    print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
        "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
        "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
        "\n",
        "    return output_dir"
      ],
      "metadata": {
        "id": "sTVMlUy1XY1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 9: Chatbot Interface**"
      ],
      "metadata": {
        "id": "uYWOrh3w2ue-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_chatbot(model_path):\n",
        "    print(\"\\nInitializing chatbot...\")\n",
        "    chatbot = EnhancedChatbot(model_path)\n",
        "\n",
        "    print(\"\\nLSPU Chatbot is ready! Type 'exit' or 'quit' to stop.\")\n",
        "    print(\"Type 'metrics' to see the model's performance metrics again.\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\nYou: \").strip()\n",
        "\n",
        "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "            print(\"Goodbye! Thank you for using the LSPU Student Handbook Chatbot.\")\n",
        "            break\n",
        "\n",
        "        if user_input.lower() == \"metrics\":\n",
        "            data = load_and_preprocess_data('LSPU-Student-Handbook-dataset.json')\n",
        "            datasets = create_dataset(data)\n",
        "            metrics = evaluate_model(chatbot, datasets['test'])\n",
        "            print(\"\\nModel Metrics:\")\n",
        "            print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
        "            print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
        "            print(f\"Recall: {metrics['recall']:.4f}\")\n",
        "            print(f\"Precision: {metrics['precision']:.4f}\")\n",
        "            continue\n",
        "\n",
        "        response = chatbot.generate_response(user_input)\n",
        "        print(f\"Bot: {response}\")"
      ],
      "metadata": {
        "id": "6mKjn0mk2uq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 10: Main Execution**"
      ],
      "metadata": {
        "id": "IB67rTTA2wX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    print(\"LSPU Student Handbook Chatbot System\")\n",
        "    print(\"====================================\")\n",
        "\n",
        "    model_path = \"./enhanced-t5-chatbot\"\n",
        "\n",
        "    if not os.path.exists(model_path):\n",
        "        print(\"Model not found. Training and saving the model...\")\n",
        "        model_path = train_and_evaluate()\n",
        "    else:\n",
        "        print(\"Loading pre-trained model...\")\n",
        "\n",
        "    run_chatbot(model_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "wkBU9vCufaJs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}